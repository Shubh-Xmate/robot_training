{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wU6mDI0qKXgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check"
      ],
      "metadata": {
        "id": "pLvNgAHjB1Bz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74jG74gQLYwA",
        "outputId": "489d62d3-f8c8-4c7f-b576-61022ae90706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing relevant libraries and taking uploaded data from github"
      ],
      "metadata": {
        "id": "t8o-FS65B5IG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NCjWYErfLaiJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import losses as lo_ss\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rq8wV53OPNNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "175d4740-7428-4909-f1d1-b75d53524d67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0         1     2     3     4            5            6            7    \\\n",
              "0       2  0.000027 -18.8   1.0 -51.2  5928.833355   796.358239 -2237.601612   \n",
              "1       0  0.000030 -18.8   1.0 -51.2  -861.308727  2307.828939  1422.328418   \n",
              "2       0  0.000034 -18.7   1.0 -50.6 -1851.188870 -1823.489706 -1483.762355   \n",
              "3       0  0.000038 -18.5   1.0 -50.0  3826.693181  3614.186279 -1810.579042   \n",
              "4       0  0.000042 -18.4   1.0 -49.4 -2345.665781 -1009.244748 -2798.268228   \n",
              "...   ...       ...   ...   ...   ...          ...          ...          ...   \n",
              "4030    2  0.000000  82.9  31.8 -61.4 -1245.734872 -1615.032049 -1784.158416   \n",
              "4031    2  0.000000  82.7  31.8 -61.4  1743.167881  -219.969994   353.010067   \n",
              "4032    0  0.000000  82.5  31.8 -61.3  7523.643927 -1346.518049  -437.245974   \n",
              "4033    2  0.000000  82.5  31.8 -61.4 -1639.431817  3837.660828  -218.350574   \n",
              "4034    2  0.000000  82.5  31.8 -61.4 -1687.918422 -1416.745916  1508.833009   \n",
              "\n",
              "              8            9    ...         145         146         147  \\\n",
              "0    -4291.285903   485.614457  ... -184.665696 -143.541755 -356.073963   \n",
              "1    -2551.334533  -561.570173  ...  270.453742  -52.096877  195.996829   \n",
              "2      956.114350 -2017.286057  ...  193.869011   94.069003 -126.387084   \n",
              "3     -228.922261  -270.595184  ... -174.377444 -143.659035  -24.409795   \n",
              "4       87.699940   326.821614  ...   64.504130 -210.224006  -65.296202   \n",
              "...           ...          ...  ...         ...         ...         ...   \n",
              "4030   361.415756   156.039717  ...   -3.633283  -99.510990   52.122663   \n",
              "4031   413.369952 -1057.733851  ... -371.606440 -161.391936   90.013115   \n",
              "4032 -3184.785512   697.115396  ...  -87.525202  -60.850698  211.654447   \n",
              "4033  2473.661181     0.514134  ...   79.065958  -12.731323  -97.202328   \n",
              "4034 -3202.899478   479.292536  ...   56.731513   52.588404  -62.325078   \n",
              "\n",
              "             148         149         150         151         152         153  \\\n",
              "0    -155.953441 -571.260816  548.824942 -147.125237  -15.699306 -114.852994   \n",
              "1    -103.884779  111.627671   19.870049  209.325186   13.936288  261.752370   \n",
              "2     145.724626  131.659001  -84.659240   90.034459  -63.858855  -79.851765   \n",
              "3       3.847896  -80.058108  -12.424300   -2.449475  -82.492338   33.375895   \n",
              "4      40.269294   24.559670  -71.945851  103.014317 -113.017795  145.268924   \n",
              "...          ...         ...         ...         ...         ...         ...   \n",
              "4030 -141.796568  168.401590 -145.657905   78.278022   38.090024 -214.492314   \n",
              "4031   56.553212   17.573522  136.672030  145.804752 -203.689696 -118.373036   \n",
              "4032 -121.017454  443.020185 -395.819706  -85.330893 -357.307181 -178.380422   \n",
              "4033  -74.081315 -277.071732   50.313883   -6.449069 -194.796294 -248.021765   \n",
              "4034 -236.009245  256.691281   72.667507  405.343475 -174.863390  158.616327   \n",
              "\n",
              "             154  \n",
              "0    -125.048631  \n",
              "1      15.264500  \n",
              "2     -36.496837  \n",
              "3      10.991025  \n",
              "4    -112.158828  \n",
              "...          ...  \n",
              "4030  145.345199  \n",
              "4031   52.921203  \n",
              "4032   75.121322  \n",
              "4033 -117.951381  \n",
              "4034   23.589513  \n",
              "\n",
              "[4035 rows x 155 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17e498d9-3aed-41f5-9424-df45d33caf47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-18.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-51.2</td>\n",
              "      <td>5928.833355</td>\n",
              "      <td>796.358239</td>\n",
              "      <td>-2237.601612</td>\n",
              "      <td>-4291.285903</td>\n",
              "      <td>485.614457</td>\n",
              "      <td>...</td>\n",
              "      <td>-184.665696</td>\n",
              "      <td>-143.541755</td>\n",
              "      <td>-356.073963</td>\n",
              "      <td>-155.953441</td>\n",
              "      <td>-571.260816</td>\n",
              "      <td>548.824942</td>\n",
              "      <td>-147.125237</td>\n",
              "      <td>-15.699306</td>\n",
              "      <td>-114.852994</td>\n",
              "      <td>-125.048631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-18.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-51.2</td>\n",
              "      <td>-861.308727</td>\n",
              "      <td>2307.828939</td>\n",
              "      <td>1422.328418</td>\n",
              "      <td>-2551.334533</td>\n",
              "      <td>-561.570173</td>\n",
              "      <td>...</td>\n",
              "      <td>270.453742</td>\n",
              "      <td>-52.096877</td>\n",
              "      <td>195.996829</td>\n",
              "      <td>-103.884779</td>\n",
              "      <td>111.627671</td>\n",
              "      <td>19.870049</td>\n",
              "      <td>209.325186</td>\n",
              "      <td>13.936288</td>\n",
              "      <td>261.752370</td>\n",
              "      <td>15.264500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>-18.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-50.6</td>\n",
              "      <td>-1851.188870</td>\n",
              "      <td>-1823.489706</td>\n",
              "      <td>-1483.762355</td>\n",
              "      <td>956.114350</td>\n",
              "      <td>-2017.286057</td>\n",
              "      <td>...</td>\n",
              "      <td>193.869011</td>\n",
              "      <td>94.069003</td>\n",
              "      <td>-126.387084</td>\n",
              "      <td>145.724626</td>\n",
              "      <td>131.659001</td>\n",
              "      <td>-84.659240</td>\n",
              "      <td>90.034459</td>\n",
              "      <td>-63.858855</td>\n",
              "      <td>-79.851765</td>\n",
              "      <td>-36.496837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>-18.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-50.0</td>\n",
              "      <td>3826.693181</td>\n",
              "      <td>3614.186279</td>\n",
              "      <td>-1810.579042</td>\n",
              "      <td>-228.922261</td>\n",
              "      <td>-270.595184</td>\n",
              "      <td>...</td>\n",
              "      <td>-174.377444</td>\n",
              "      <td>-143.659035</td>\n",
              "      <td>-24.409795</td>\n",
              "      <td>3.847896</td>\n",
              "      <td>-80.058108</td>\n",
              "      <td>-12.424300</td>\n",
              "      <td>-2.449475</td>\n",
              "      <td>-82.492338</td>\n",
              "      <td>33.375895</td>\n",
              "      <td>10.991025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>-18.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-49.4</td>\n",
              "      <td>-2345.665781</td>\n",
              "      <td>-1009.244748</td>\n",
              "      <td>-2798.268228</td>\n",
              "      <td>87.699940</td>\n",
              "      <td>326.821614</td>\n",
              "      <td>...</td>\n",
              "      <td>64.504130</td>\n",
              "      <td>-210.224006</td>\n",
              "      <td>-65.296202</td>\n",
              "      <td>40.269294</td>\n",
              "      <td>24.559670</td>\n",
              "      <td>-71.945851</td>\n",
              "      <td>103.014317</td>\n",
              "      <td>-113.017795</td>\n",
              "      <td>145.268924</td>\n",
              "      <td>-112.158828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4030</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.9</td>\n",
              "      <td>31.8</td>\n",
              "      <td>-61.4</td>\n",
              "      <td>-1245.734872</td>\n",
              "      <td>-1615.032049</td>\n",
              "      <td>-1784.158416</td>\n",
              "      <td>361.415756</td>\n",
              "      <td>156.039717</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.633283</td>\n",
              "      <td>-99.510990</td>\n",
              "      <td>52.122663</td>\n",
              "      <td>-141.796568</td>\n",
              "      <td>168.401590</td>\n",
              "      <td>-145.657905</td>\n",
              "      <td>78.278022</td>\n",
              "      <td>38.090024</td>\n",
              "      <td>-214.492314</td>\n",
              "      <td>145.345199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4031</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.7</td>\n",
              "      <td>31.8</td>\n",
              "      <td>-61.4</td>\n",
              "      <td>1743.167881</td>\n",
              "      <td>-219.969994</td>\n",
              "      <td>353.010067</td>\n",
              "      <td>413.369952</td>\n",
              "      <td>-1057.733851</td>\n",
              "      <td>...</td>\n",
              "      <td>-371.606440</td>\n",
              "      <td>-161.391936</td>\n",
              "      <td>90.013115</td>\n",
              "      <td>56.553212</td>\n",
              "      <td>17.573522</td>\n",
              "      <td>136.672030</td>\n",
              "      <td>145.804752</td>\n",
              "      <td>-203.689696</td>\n",
              "      <td>-118.373036</td>\n",
              "      <td>52.921203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4032</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.5</td>\n",
              "      <td>31.8</td>\n",
              "      <td>-61.3</td>\n",
              "      <td>7523.643927</td>\n",
              "      <td>-1346.518049</td>\n",
              "      <td>-437.245974</td>\n",
              "      <td>-3184.785512</td>\n",
              "      <td>697.115396</td>\n",
              "      <td>...</td>\n",
              "      <td>-87.525202</td>\n",
              "      <td>-60.850698</td>\n",
              "      <td>211.654447</td>\n",
              "      <td>-121.017454</td>\n",
              "      <td>443.020185</td>\n",
              "      <td>-395.819706</td>\n",
              "      <td>-85.330893</td>\n",
              "      <td>-357.307181</td>\n",
              "      <td>-178.380422</td>\n",
              "      <td>75.121322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4033</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.5</td>\n",
              "      <td>31.8</td>\n",
              "      <td>-61.4</td>\n",
              "      <td>-1639.431817</td>\n",
              "      <td>3837.660828</td>\n",
              "      <td>-218.350574</td>\n",
              "      <td>2473.661181</td>\n",
              "      <td>0.514134</td>\n",
              "      <td>...</td>\n",
              "      <td>79.065958</td>\n",
              "      <td>-12.731323</td>\n",
              "      <td>-97.202328</td>\n",
              "      <td>-74.081315</td>\n",
              "      <td>-277.071732</td>\n",
              "      <td>50.313883</td>\n",
              "      <td>-6.449069</td>\n",
              "      <td>-194.796294</td>\n",
              "      <td>-248.021765</td>\n",
              "      <td>-117.951381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4034</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>82.5</td>\n",
              "      <td>31.8</td>\n",
              "      <td>-61.4</td>\n",
              "      <td>-1687.918422</td>\n",
              "      <td>-1416.745916</td>\n",
              "      <td>1508.833009</td>\n",
              "      <td>-3202.899478</td>\n",
              "      <td>479.292536</td>\n",
              "      <td>...</td>\n",
              "      <td>56.731513</td>\n",
              "      <td>52.588404</td>\n",
              "      <td>-62.325078</td>\n",
              "      <td>-236.009245</td>\n",
              "      <td>256.691281</td>\n",
              "      <td>72.667507</td>\n",
              "      <td>405.343475</td>\n",
              "      <td>-174.863390</td>\n",
              "      <td>158.616327</td>\n",
              "      <td>23.589513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4035 rows Ã— 155 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17e498d9-3aed-41f5-9424-df45d33caf47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17e498d9-3aed-41f5-9424-df45d33caf47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17e498d9-3aed-41f5-9424-df45d33caf47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "github_raw_link=\"https://raw.githubusercontent.com/Aryansh085/LAP/main/858417d1-7d54-4115-a01b-fdda5e03ada3_testing_combined_rows4035_disc_1_0p9_MULTIPLY_preproces_155cols.csv\"\n",
        "df=pd.read_csv((github_raw_link),header = None)\n",
        "\n",
        "df\n",
        "\n",
        "# df = pd.read_csv('/content/858417d1-7d54-4115-a01b-fdda5e03ada3_testing_combined_rows4035_disc_1_0p9_MULTIPLY_preproces_155cols.csv')\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkK35zDcQqSN"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NPsxu1WQEYz",
        "outputId": "cac19635-6686-4b13-f817-e72ee766e668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\n",
            "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\n"
          ]
        }
      ],
      "source": [
        "print(list(df))\n",
        "\n",
        "cols = []\n",
        "cols.append(0);\n",
        "\n",
        "for i in range(5, 155):\n",
        "  cols.append(i)\n",
        "\n",
        "# cols = list(df)[5:]\n",
        "# for i in range(0,1):\n",
        "#   cols.append(df[df.columns[i]].to_list())\n",
        "# for i in range(5,155):\n",
        "#   cols.append(df[df.columns[i]].to_list())\n",
        "\n",
        "#2-5 columns are not used in training. \n",
        "print(cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yTAdQwmQC65"
      },
      "outputs": [],
      "source": [
        "df_for_training = df[cols].astype(float)\n",
        "\n",
        "# df_for_plot=df_for_training.tail(5000)\n",
        "# df_for_plot.plot.line()\n",
        "\n",
        "# LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
        "# normalize the dataset\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(df_for_training)\n",
        "df_for_training_scaled = scaler.transform(df_for_training)\n",
        "\n",
        "\n",
        "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
        "#In this example, the n_features is 1+150. We will make timesteps = 1 (past days data used for training). \n",
        "\n",
        "#Empty lists to be populated using formatted training data\n",
        "trainX = []\n",
        "trainY = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY2qmpctPNTP",
        "outputId": "b92f3d4a-b1f5-42f4-e138-4bbd254a7e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX shape == (4030, 5, 150).\n",
            "trainY shape == (4030, 1).\n"
          ]
        }
      ],
      "source": [
        "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
        "n_past = 5  # Number of past days we want to use to predict the future.\n",
        "\n",
        "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
        "#In my example, my df_for_training_scaled has a shape (4030, 151)\n",
        "#4030 refers to the number of data points and 151 refers to the columns (multi-variables).\n",
        "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
        "    trainX.append(df_for_training_scaled[i - n_past:i, 1:df_for_training.shape[1]])\n",
        "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
        "    # trainY.append(df_for_training_scaled[i - n_past:i, 0])\n",
        "\n",
        "trainX, trainY = np.array(trainX), np.array(trainY)\n",
        "\n",
        "print('trainX shape == {}.'.format(trainX.shape))\n",
        "print('trainY shape == {}.'.format(trainY.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model making starts here "
      ],
      "metadata": {
        "id": "RtTd1rycBkU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8DajKZ3Rxl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "bf28ed3b-c7b1-4f92-e92b-d1a2cf19f3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 5, 64)             55040     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 32)             12416     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 150)               109800    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 150)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 151       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 177,407\n",
            "Trainable params: 177,407\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "3627/3627 [==============================] - 43s 11ms/step - loss: 0.9428 - accuracy: 0.0000e+00 - val_loss: 1.1753 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "3627/3627 [==============================] - 32s 9ms/step - loss: 0.6375 - accuracy: 0.0000e+00 - val_loss: 1.3117 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "3627/3627 [==============================] - 32s 9ms/step - loss: 0.4297 - accuracy: 0.0000e+00 - val_loss: 1.2175 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "3627/3627 [==============================] - 33s 9ms/step - loss: 0.3067 - accuracy: 0.0000e+00 - val_loss: 1.2700 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "3627/3627 [==============================] - 34s 9ms/step - loss: 0.2286 - accuracy: 0.0000e+00 - val_loss: 1.4064 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "3627/3627 [==============================] - 33s 9ms/step - loss: 0.1576 - accuracy: 0.0000e+00 - val_loss: 1.4198 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "3627/3627 [==============================] - 33s 9ms/step - loss: 0.1241 - accuracy: 0.0000e+00 - val_loss: 1.4704 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "3627/3627 [==============================] - 32s 9ms/step - loss: 0.1096 - accuracy: 0.0000e+00 - val_loss: 1.4713 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "3627/3627 [==============================] - 32s 9ms/step - loss: 0.0926 - accuracy: 0.0000e+00 - val_loss: 1.4332 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "3627/3627 [==============================] - 32s 9ms/step - loss: 0.0869 - accuracy: 0.0000e+00 - val_loss: 1.5640 - val_accuracy: 0.0000e+00\n",
            "<keras.callbacks.History object at 0x7f4596940a50>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f459683a650>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(32, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(150, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=False))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
        "# model.compile(optimizer='adam',loss = lo_ss.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "history=model.fit(trainX, trainY, epochs=10, batch_size=1,validation_split=0.1, verbose=1)\n",
        "\n",
        "# plt.scatter(trainY,history)\n",
        "print(history)\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQkZ9WAGX8Hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3444a44e-010b-4732-fadf-fc6ff7540a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126/126 [==============================] - 2s 7ms/step - loss: 0.2021 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20208536088466644, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# model.model.evaluate(X_test_flat,Y_test)\n",
        "model.evaluate(trainX, trainY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same model just changing the activation function and parameters"
      ],
      "metadata": {
        "id": "xaHYIeGiBvRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wLDpJA2bRI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5681bf06-d929-4f9a-a1dc-cb1f2836b579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_21 (LSTM)              (None, 5, 100)            100400    \n",
            "                                                                 \n",
            " lstm_22 (LSTM)              (None, 5, 50)             30200     \n",
            "                                                                 \n",
            " lstm_23 (LSTM)              (None, 150)               120600    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 151       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 251,351\n",
            "Trainable params: 251,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "3627/3627 [==============================] - 35s 9ms/step - loss: 1.0043 - accuracy: 0.0000e+00 - val_loss: 1.0019 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "3627/3627 [==============================] - 31s 9ms/step - loss: 1.0131 - accuracy: 0.0000e+00 - val_loss: 1.0028 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "3627/3627 [==============================] - 31s 9ms/step - loss: 0.9854 - accuracy: 0.0000e+00 - val_loss: 1.0106 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "3627/3627 [==============================] - 31s 8ms/step - loss: 0.9100 - accuracy: 0.0000e+00 - val_loss: 1.0242 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "3627/3627 [==============================] - 31s 8ms/step - loss: 0.7242 - accuracy: 0.0000e+00 - val_loss: 1.1286 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "3627/3627 [==============================] - 29s 8ms/step - loss: 0.4261 - accuracy: 0.0000e+00 - val_loss: 1.2600 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "3627/3627 [==============================] - 31s 8ms/step - loss: 0.2826 - accuracy: 0.0000e+00 - val_loss: 1.2963 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "3627/3627 [==============================] - 30s 8ms/step - loss: 0.2155 - accuracy: 0.0000e+00 - val_loss: 1.3081 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "3627/3627 [==============================] - 31s 8ms/step - loss: 0.1846 - accuracy: 0.0000e+00 - val_loss: 1.2667 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "3627/3627 [==============================] - 30s 8ms/step - loss: 0.1517 - accuracy: 0.0000e+00 - val_loss: 1.1884 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fed7e85c310>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(150, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=False))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "model.fit(trainX, trainY, epochs=10, batch_size=1, validation_split=0.1, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMkiPLKZbY6R"
      },
      "outputs": [],
      "source": [
        "model.evaluate(trainX, trainY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTwjhx5Hcld5"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(150, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=False))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "model.fit(trainX, trainY, epochs=15, batch_size=1, validation_split=0.1, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktjEzIMwekVC"
      },
      "outputs": [],
      "source": [
        "model.evaluate(trainX, trainY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nVU8pxTfJju"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(150, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=False))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(100))\n",
        "\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "history=model.fit(trainX, trainY, epochs=5, batch_size=1, validation_split=0.1, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# plt.scatter(trainY,history)\n",
        "print(history)\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adPRt0migRSb"
      },
      "outputs": [],
      "source": [
        "model.evaluate(trainX, trainY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RD933_SgR2r"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(150, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=False))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(100,activation='sigmoid'))\n",
        "\n",
        "model.add(Dense(trainY.shape[1],activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "history=model.fit(trainX, trainY, epochs=5, batch_size=1, validation_split=0.1, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# plt.scatter(trainY,history)\n",
        "print(history)\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiFR-WJ4gYbh"
      },
      "outputs": [],
      "source": [
        "model.evaluate(trainX, trainY)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gIDtKkK_xdqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pLvNgAHjB1Bz",
        "t8o-FS65B5IG",
        "kkK35zDcQqSN",
        "RtTd1rycBkU9",
        "xaHYIeGiBvRc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}